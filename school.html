<!DOCTYPE html>

<html lang ="en">

    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0"> <!-- make sure that the content fit the device on which we open -->
        <title> School </title>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Audiowide|Sofia|Trirong|Arial|Open+Sans">
        <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
        <script src="js/script.js"></script>
        <link rel="stylesheet" href="generic-patterns/css/main.css">
        <link rel="stylesheet" href="generic-patterns/css/horizontalmenu.css">
        <link rel="stylesheet" href="generic-patterns/css/footer.css">
        <link rel="stylesheet" href="css/school.css">
    </head>

    <!-- Within de web page -->
    <body>
        <div id="horizontalmenu"></div>
        
        <section class="main-container">

            <a href="https://www.enib.fr/fr/" class="enib-link"><img src="images/enib.webp" class="enib-logo"></a>

            <section class="accordion">

                <h1 class="title">4th year of engineering studies</h1>
                <p>In this section, I try to provide an overview of my learning outcomes by presenting the most relevant parts of each course. 
                    To have a broader view of each course's content, you can expand their section by clicking on their title.</p>
                
                <div class="sub-container"> <div class="label"> <h2>Signal Processing</h2> </div>
                    <div class="keywords">Convolution systems, FIR & IIR filters, distorsion, correlation, Butterworth & Chebychev, Laplace/Fourier transforms, Analog/Discrete signals, modulation, Matlab</div>
                    <div class="content">
                        <p>The following content gives an overview of the digital/analog signal processing courses that I followed at my school. This is not meant to be a comprhensive description but rather a brief guide.</p>
                        <hr>
                        <div class="subject-subdiv"> <h3>1. Convolution</h3>
                            <p>In linear filtering, convolution plays a fundamental role. Indeed, if we write a signal \(x(t)\) such that \(x(t) = \int^{+\infty}_{-\infty} x(\tau)\delta(t-\tau) d\tau \),
                                with \( \delta(t) \) the Dirac impulse. Then when applying the filter function on the signal, we have :
                                \[ f(x) = \int^{+\infty}_{-\infty} f(x(\tau)\delta(t-\tau)) d\tau \]
                                \[ \text{Linearity property: } f(x) = \int^{+\infty}_{-\infty} x(\tau)f(\delta(t-\tau)) d\tau \]
                                \[ \text{Time-independent property: } y(t) = \int^{+\infty}_{-\infty} x(\tau) h((t-\tau)) d\tau \]
                                \(h(t)\) represents the output function when we input the Dirac impulse into the filter and \( y(t) \) is the signal \( x(t) \) filtered.
                                We end up on the convolution operation between the input signal and the Dirac impulse output function.
                            </p>
                        </div>
                        <hr>
                        <div class="subject-subdiv"> <h3>2. Physical limitations and signal reconstitution</h3>
                            <p>To simplify calculations, physicians invented this mathematical object called Dirac impulse. Unfortunately, in the real world we cannot recreate this impulse.
                                Indeed, transistors have a toggle limitation due to the speed of current, causing a Dirac impulse to look like more of a brief door function.
                                But what does this actually involve for a signal? Because to sample a continuous signal, we need to use this theorical object. Does this inaccuracy cause the signal to transform?
                                
                            </p>
                        </div>
                        <hr>
                        <div class="subject-subdiv"> <h3>3. Discrete Fourier Transform (DFT) and Analog Filter Synthesis</h3>
                            <p>This course gives an understanding on how we numerize a signal, what are the impacts and how we can elaborate a digital filter to filter an analog signal.</p>
                            <p>With a view to processing digital signals, we need to turn a continuous (or analog) signal into a discrete signal
                                that consists of a finite number of samples that can be handled by a processor. Therefore, we use the DFT, let's remind us quickly of that. </p>
                            <div class="frame-highlight">
                                <ol>
                                    <li> <span class="bold">Sampling & Quantification:</span>            turns the continuous signal into a discrete signal.</li>
                                    <li> <span class="bold">Temporal truncation of the signal:</span>    get an finite amount of samplings.</li>
                                    <li> <span class="bold">Spectral discretization:</span>              turns the continuous spectrum into a discrete spectrum.</li>
                                    <li> <span class="bold">Zero-Padding (optional):</span>              increase the spectrum's resolution.</li>
                                </ol>
                            </div>
                            <p>However, each one of these steps has an impact on the signal. Indeed, discretizing a signal is not without consequences and some parameters should be chosen wisely
                                to avoid aliasing and other artifacts.
                            </p>
                            <button id="tsi-graphs1">Show illustrations</button>
                        </div>
                        <hr>
                        <div class="subject-subdiv"> <h3>4. Digital Filter Synthesis</h3>
                            <p>There exists 2 types of digital filters: <span class="mark">IIR (Infinite Impulse Response)</span> and <span class="mark">FIR (Finite Impulse Response)</span>.</p>
                            <div class="frame-highlight">
                            <span class="mark">IIR:</span> the output depends on the current and previous inputs and outputs <span class="math">\( s[n] = \sum_{i=0}^{M-1}{a_i e[i]} + \sum_{j=0}^{N-1}{a_i s[n-i]} \)</span>
                                    <p>The purpose of this method is to determine the equivalent digital form \( H(z) \) (\(z\)-domain) of the analog template filter \( H(p) \). From there, we can deduce the <span class="bold">recursive algorithm \( s[n] \)</span> 
                                        that will implement the digital filter.
                                        This method is <span class="bold">more accurate</span> and will be generally preferred over the RIF method. However, the analog reference filter must meet some stability criteria (poles on the left half part of the complex plane).</p>
                                    <p> - <span class="bold">Invariance of the Impulse Response:</span> This method consists of starting with a digital filter template and then switch to an analog template by considering that \( \omega_n = \omega_a \).
                                        Then we figure out the analog filter's transfer function (analog poles \(p)\) and we match back this analog filter with the correspondong digital filter. </p>
                                    </p>
                                    <p> - <span class="bold">Bilinear transform:</span> This method consists of the same approach as the previous one expects we consider that digital pulsations are transformed to analog pulsations by the following relation:
                                        \( \omega_a = \frac{2}{T_e} tan(\frac{\omega_n T_e}{2}) \). This relation comes from this approximation: \( p \equiv \frac{2}{T_e}\frac{(1-z^{-1})}{(1+z^{-1})} \).
                                        <img src="images/bilinear.svg" alt="bilinear">
                                    </p>
                                    <p>All in all, this method has some benefits - <span class="bold">more selective, faster and requires fewer memory registers</span> - and some drawbacks - <span class="bold">can be unstable (poles), non-linear phase</span>.</p>
                            </div>
                            <div class="frame-highlight">
                                <span class="mark">FIR:</span> the output depends only on the current and previous inputs > <span class="math">\( s[n] = \sum_{i=0}^{N-1}{a_i e[i] h[n-i]} \)</span>
                                <p>The purpose of this method is to determine an <span class="bold">approximation of the filter's impulse response</span> and then convolute it with the input signal.
                                    As we consider only an approximation, the <span class="bold">filter's characteristics get modified</span> and we have to deal with some drawbacks such as the 
                                    <span class="bold">apparition of wavelets</span> in the filter's spectrum (caused by the truncation --> convolution with sinc).</p>
                                <p> - <span class="bold">Coherence of the Impulse Response:</span></p>
                                    <ol>
                                        <li>Sampling frequency: Determine \( f_{max}\) and apply the Shannon theorem: \(f_e\ \geq\ 2f_{max}\)</li>
                                        <li>Impulse response (IR): Find the impulse response of the analog filter.</li>
                                        <li>Signal truncation: Truncate the IR after a rank N such that \( h(N) \geq \frac{h_{max}}{100}\)</li>
                                    </ol>
                                <p> - <span class="bold">Discretization of the inverse DFT:</span> We determine the IR samples using direclty the inverse DFT definition > <span class="math">\( h[n] = \frac{1}{N} \sum_{k=-\frac{N}{2}}^{\frac{N}{2}-1} X(k\delta f) e^{j2\pi \frac{n}{N}k} \)</span>.</p>
                                <p>All in all, this method has some benefits - <span class="bold">always stable (no poles), linear phase (if symmetric)</span> - and some drawbacks - <span class="bold">less selective, less efficient and requires more memory space</span>.</p>
                            </div>
                        </div>
                        <hr>
                        <div class="subject-subdiv"> <h3>5. Signal modulation</h3>
                            <div> <h4>Concept</h4>
                                <p>The purpose of <span class="bold">signal modulation</span> is to use (or <span class="bold">modulate</span>) some signal characteristics (modulation, phase ...) to <span class="bold">encode binary data</span> and transmit it.
                                    On one end of the communication, the <span class="bold">transmitter</span> modulates the signal. On the other end, the <span class="bold">receiver</span> demodulates the signal (i.e., deduce the data from the signal characteristics).
                                </p>
                                <p>When it comes to transmitting information using signals, we face some issues. For that matter, there exists a wide variety of modulation techniques:</p>
                                <div class="frame-highlight">
                                    <ul>
                                        <li>if signals use the same range of frequencies to transmit data, we would end up having all signals interfering with each other (signal overlapping)</li>
                                        <li>to detect rather low frequency signals (like voice), the size of the receiver should also be ridiculously large (several kilometers)</li>
                                    </ul>
                                </div>
                            </div>
                            <div> <h4>ASK technique</h4>
                                <p>ASK Modulation slides the signal spectrum along the frequency axis to a higher frequency, thereby countering the two previous problems. Indeed, we can control the frequency at which we want our signal to be broadcasted, thereby
                                    <span class="mark">reducing significantly the size of the receiver</span> and also <span class="mark">avoiding signal overlapping</span>.</p>
                                <p>In the following schema, \(x(t)\) is the signal to be transmitted (carrier signal), \(s(t)\) is the signal modulated and emitted,
                                    \(m(t)\) is the modulation signal, \(d(t)\) is the demodulation signal and \(y(t)\) is ideally the same as \(x(t)\).
                                </p>
                                <img class="modulation-img" src="images/modulation.drawio.png" alt="mod">
                            </div>
                            <div> <h4>Other types of modulations</h4>
                                <p>We have briefly covered some modulations method other than the ASK (Amplitude Shift Keying) method. There are notably:</p>
                                    <div class="frame-highlight">
                                        <ul>
                                            <li><span class="bold">QAM (Quadrature Amplitude Modulation):</span> the carrier signal is modulated by a binary signal (0 or 1) that is represented by 2 different amplitudes.</li>
                                            <li><span class="bold">QPSK (Quadrature Phase Shift Keying):</span> the carrier signal is modulated by a binary signal (0 or 1) that is represented by 2 different phases.</li>
                                            <li><span class="bold">OFDM (Orthogonal Frequency Division Multiplexing):</span> the information is carried by a multi-carrier signal.</li>
                                        </ul>
                                    </div>
                                <p><a href="signal.html">Here</a> I go more in details.</p>
                            </div>
                        </div>
                        <hr>
                        <div class="subject-subdiv"> <h3>6. Signal correlation</h3>
                            <p>The <span class="bold">correlation</span> method is a technique that consists of multiplying values of 2 signals at each time instant and summing the results.
                                This is a well known technique in signal processing to identify <span class="bold">similarities</span> between 2 signals or even the <span class="bold">presence</span> of a signal in another one.
                                This can be viewed as a <span class="bold">scalar product</span> between 2 signals. The formula is as follows:
                                \[ r_{xy} = \int^{+\infty}_{-\infty} x(t)y(t) dt \equiv \sum^{+\infty}_{-\infty} x[n]y[n] \]
                                where \(x(t)\) is the original signal and \(y(t)\) is the signal we want to identify in \(x(t)\) (of course they are interchangeable). The result of this operation is a scalar value \(C_{xy}\) which tells us how much both signals are <span class="bold">correlated</span>.
                                However, the resulting value on its own does not mean much. It is a <span class="bold">relative</span> value not an <span class="bold">absolute</span>. That means that we need to compare it to other values to draw conclusions.
                                
                            </p>
                            <div class="frame-highlight">
                                <p>Some <span class="bold">transforms</span> are based upon this technique so as to identify features of a signal:</p>
                                <ul>
                                    <li>Fourier transform identifies frequencies (pure sinusoïd signals).</li>
                                    <li>Laplace transform measures also frequencies contributions, plus decreasing or increasing amplitudes.</li>
                                </ul>
                                <p>This is done by switching to another domain (frequency domain) where the signal is represented by a function of the frequency.</p>
                            </div>
                            <div> <h5>Auto-correlation</h5>
                                <p>The <span class="bold">auto-correlation</span> process is used for localizing or identifying one signal in an noisy environment: \( r_{xx}(\tau) = \int^{+\infty}_{-\infty} x(t)x(\tau+t) dt \).</p>
                                <div class="frame-highlight">
                                    <p>For <span class="bold">finite-energy</span> signals, their <span class="bold">energy</span> is localized at \(\tau=0\): \( W = r_{xx}(0) = \int^{+\infty}_{-\infty} x(t)^2 dt \).</p>
                                    <p>For <span class="bold">periodic (infinite energy)</span> signals, their <span class="bold">mean power</span> is also localized at \(\tau=0\): \( P = r_{xx}(0) = \frac{1}{T} \int^{+\infty}_{-\infty} x(t)^2 dt \).</p>
                                </div>
                            </div>
                            <div> <h5>Inter-correlation</h5>
                                <p>This technique is mostly used in the field of radars. When emitting a signal, we want to be able to detect if the signal comes back to us (<span class="bold">obstacle detection</span> for instance).
                                    Thus, the detector constantly needs to <span class="bold">compare</span> any received signal with the one that was originally emitted. The <span class="bold">inter-correlation</span> method also derives from the correlation concept.
                                    It is a way to identify one signal in another one. It is defined as follows:
                                    \[ r_{xy}(\tau) = \int^{+\infty}_{-\infty} x(t)y(\tau+t) dt \]
                                    where \(x(t)\) is the original signal and \(y(t)\) is the received signal that we compare to \(x(t)\). The result of this operation is a function of the delay \(\tau\).
                                </p>
                                <div class="frame-highlight">
                                    <p>Note that this operation can be written using the convolution operator \( r_{xy}(t) = ( x(\tau) * y(-\tau) )(t) \).</p>
                                </div>
                                <p>A <span class="bold">correlation</span> between the two signals, results in a <span class="bold">peak</span> at the time delay \(\tau_0\) where the two signals are the <span class="bold">most similar</span>. \(\tau_0\) corresponds to the <span class="bold">propagation time</span> of the signal.
                                    This delay can notably be used to <span class="bold">compute the distance</span> between the obstacle and the emitter.
                                </p>
                                <div class="frame-highlight">
                                    <p>If both signals are <span class="bold">periodic</span>, the inter-correlation function is also <span class="bold">periodic</span>. Therefore, we have to be careful because we could detect a <span class="bold">peak at the wrong delay</span>, resulting in wrong conclusions about the propagation time of the obstacle.</p>
                                </div>
                                <div class="frame-highlight">
                                    <p>The parameter of the inter-correlation function is not the frequency as opposed to the FT but it is the delay parameter \(\tau\).</p>
                                </div>
                            </div>
                        </div>
                        <hr>
                        <div class="subject-subdiv"> <h3>7. Further considerations</h3>
                            <p>Since I am deeply interested in that topic. I continued to learn more concepts about signal processing. On this <a href="signal.html">page</a>, I expand on that topic.
                                I aim to learn about:
                            </p>
                            <div class="frame-highlight">
                                <ol>
                                    <li>Signal encoding and encryption;</li>
                                    <li>Some modulation techniques such as QAM16, OFDM and QPSK;</li>
                                    <li>Hilbert transform and its concrete applications;</li>
                                    <li>Some communication protocols on the physical layer such as 4G.</li>
                                    <li>And more ...</li>
                                </ol>
                            </div>
                            <p>I have also started to learn about the <a href="informationtheory.html">Information theory</a>.</p>
                        </div>
                    </div>
                </div>
               
                <div class="sub-container"> <div class="label"> <h2>Image Processing</h2> </div>
                    <div class="keywords">Image representation and acquisition, pre-processing, segmentation, filtering, classification and recognition, CNN, Matlab, OpenCV</div>
                    <div class="content">
                        <p>You can find out some of my personal experiments regarding image filtering and processing <a href="currentprojects.html">here</a>. Also, you will find at the end of this page,
                            some useful Matlab and OpenCV examples. All images used in this section are taken from my Matlab labos. This section is not meant to be a comprehensive description of the course but rather summary of my understandings.
                        </p>
                        <hr>
                        <div class="subject-subdiv"> <h3>1. Introduction</h3>
                            <p>Image processing encompasses several steps with image <span class="bold">pre-processing, filtering, classification</span> and so forth. Eventually, image processing can lead to training a machine learning algorithm which aims to recognize classes of images
                                in order to automate <span class="bold">image analysis</span> and <span class="bold">decision making</span>. It is important to note that, we increase the dimension by one (regarding signal processing) and we do not deal with the time coordonate \(t\) anymore
                                but with spatial coordonates represented by \(x\) and \(y\).
                            </p>
                        </div>
                        <hr>
                        <div class="subject-subdiv"> <h3>2. Textures</h3>
                            <p>In this section, we focus on how to classify textures. Textures are hard to define as they highly depend on the context. The purpose here is to be able to detect 
                                 and classify textures by identifying patterns. To do so, we can use two approaches: <span class="bold">statistical</span> or <span class="bold">spectral</span>.
                            </p>
                            <div> <h4>Spectral approach</h4>
                                <p>The <span class="bold">spectral</span> approach consists of computing the Fourier transform of the image. Now, let me introduce the Fourier transform of a 2D signal.
                                    \[ I(u,v) = \int^{+\infty}_{-\infty}\int^{+\infty}_{-\infty} I(x,y) e^{-j2\pi(ux+vy)} dxdy \]
                                    The result \( I(u,v) \) is the description of the image in the frequency domain, where \(u\) and \(v\) represent respectively the frequency along the \(x\) and \(y\) axis.
                                    Below, I show examples of images Fourier transform. Each point on the spectrum has an intensity which represents the <span class="bold">contribution of each frequency</span> along the x-axis and y-axis.
                                    This approach is useful to detect <span class="bold">periodic textures</span>. For example in the first spectrum, we notice <span class="bold">2 small disks in a diagonal</span> which means that the image
                                    is likely to have a diagonal periodicity in this direction. In the second spectrum, we notice a <span class="bold">circle</span> which means that the image is likely to have a <span class="bold">circular periodicity</span>.
                                </p>
                                <div class="illustration-horiz-4">
                                    <img src="images/ai/stris.PNG" alt="">
                                    <img src="images/ai/tf_img1.PNG" alt="">
                                </div>
                                <div class="illustration-horiz-4">
                                    <img src="images/ai/grain.PNG" alt="">
                                    <img src="images/ai/tf_img2.PNG" alt="">
                                </div>
                                <p>With this approach, we only seek to modify the image's module (in the frequency domain). Indeed, modifying the module results in changing the looking of objects (contrast, luminosity, etc.)
                                    but <span class="bold">NOT the shapes themselves</span>, which are encoded in the phase.
                                    \[ I(u,v) = |I(u,v)| e^{j\phi(u,v)} \]
                                </p>
                            </div>
                            <div> <h4>Gabor filters</h4>
                                <p>There are also <span class="bold">Gabor filters</span> that are used to detect orientations in an image. They are <span class="bold">band-pass filters</span> and operate a linear filtering on the image
                                    to extract information about the frequency and the orientation. They are defined by the following equation:
                                    \[ G(x,y) = e^{-\frac{x^2+\gamma^2 y^2}{2\sigma^2}} e^{j2\pi(\omega_x x + \omega_y y + \psi)} \]
                                    <div class="frame-highlight">
                                        <p>In the frequency domain, this results in <span class="bold">shifting the mean of the gaussian curve</span> from 0 to the frequency \( \omega_x \) and \( \omega_y \).
                                            This shifting comes from the <span class="bold">convolution of in the frequency domain</span> between the gaussian and a shifted impulse.</p>
                                    </div>
                                    It can be easier to consider it as a couple of real functions, dephased of \( \frac {\pi }{2} \), the first one being the real part and the second one the imaginary part.
                                    \[ G_1(x,y) = cos(\omega_x x + \omega_y b) e^{-\frac{x^2+\gamma^2 y^2}{2\sigma^2}}\]
                                    \[ G_2(x,y) = sin(\omega_x x + \omega_y b) e^{-\frac{x^2+\gamma^2 y^2}{2\sigma^2}}\]
                                    where \( \sigma \) is the standard deviation of the Gaussian function and \(u\) and \(v\) are the frequencies along the \(x\) and \(y\) axis. By changing the value of sigma, we change the bandwidth of the filter.
                                    The more sigma is small, the less the filter is selective. The more sigma is large, the more the filter is selective. Indeed, a compression in the frequency domain is an expansion in the spatial domain.
                                </p>
                            </div>    
                            
                            <button id="textures-illustrations"> Show illustrations</button>

                            <div> <h4>Statistical approach</h4>
                                <p>The <span class="bold">statistical</span> approach consists more in considering the occurences of patterns. For example, to spot repetitive interactions between 2 pixels separated by a distance \( d \),
                                    we can compute the <span class="bold">co-occurrence matrix</span> of the image.
                                    \[
                                        \Gamma(d,\theta) = \Gamma(dx,dy) = \frac{1}{N}
                                        \left(
                                        \begin{matrix}
                                            ... & ... & ...\\
                                            ... & P(X(i,j),X(i+dx,j+dy)) & ...\\
                                            ... & ... & ... \\
                                        \end{matrix}
                                        \right).   
                                    \]
                                    \(N\) is the number of pixels in the image, \(i(x,y)\) is the intensity of the 1st pixel and \(j(x,y)\) of the 2nd one. This formula makes things look harder than they really are.
                                    This matrix simply stores the number of occurences of each pair of intensity values \( i <-> j \). This pair is defined by the distance \(d\) and the angle \(\alpha\) between the two pixels.
                                    Hence, a co-occurrence matrix with a <span class="bold">strong diagonal</span> means that the image is <span class="bold">homogeneous</span> because the diagonal is where the intensity values \(i\) and \(j\) are the same.
                                    Usually, we compute multiple co-occurrence matrices with different values of \(d\) and \(\alpha\). We can extract several characteristics from them such as:
                                </p>
                                <div class="frame-highlight">
                                    <ul>
                                        <li>Entropy: \( H(d,\theta) \)</li>
                                        <li>Energy: \( E(d,\theta) \)</li>
                                        <li>Contrast: \( C(d,\theta) \)</li>
                                        <li>Homogeneity: \( Hom(d,\theta) \)</li>
                                        <li>Correlation: \( Corr(d,\theta) \)</li>
                                    </ul>
                                </div>
                                <p>Those are compliacted math formulas and are quite tough to grasp, so I shan't expand on them. Finally, we put them in a <span class="bold">Haralick vector</span> to represent the image.
                                    This vector is a vector of those characteristics and can be used to classify the image.
                                </p>
                                <p>Of course, the statistical approach is not restricted to this co-occurrence matrix. This is simply an example. Throughout this page, I will refer to other methods that are based on this approach.</p>
                                <button id="textures-illustrations"> Show illustrations</button>
                            </div>
                            <div> <h4>Texture generation</h4>
                                <p>The <span class="mark">synthesis of textures</span> is also a common practice in image processing. It consists of <span class="bold">generating a texture from a given model</span>. The model can be a texture or a set of textures.
                                    We start with a texture A and we generate a whole image different from A but that looks the same. Neither is it as simple as a copy-paste, nor a completely random operation. There exist several methods which are not fit for the same type of textures.
                                </p>
                                <div class="frame-highlight">
                                    <ul>
                                        <li><span class="bold">Image Quilting:</span> we slice the image in rectangles and at each border and we compute the overlapping error and merge borders where this error is the least significant.</li>
                                        <li><span class="bold">Chaos Mosaic:</span> we randomly copy-paste the input texture to form an image and we apply a low-pass filter on the image.</li>
                                        <li><span class="bold">Resampling:</span> we form each pixel by one, by taking into account its surrounding patterns.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                        <hr>
                        <div class="subject-subdiv"> <h3>3. Image preprocessing</h3>
                            <p>Image preprocessing is fundamental in the field of image processing. It can save heavy computations by improving the quality of an image before processing it. </p>
                            <div> <h4>Histogram</h4>
                                <p>To describe an image, we can use a <span class="bold">histogram</span> that shows the occurences of each intensity value (it is a statistical approach). A histogram is a good indicator of the <span class="bold">dynamic</span>
                                    of an image. An example of a histogram done with Matlab is shown below. The illustration below shows an <span class="bold">anamorphosis</span> of the histogram. We started with a poorly contrasted image, applied a <span class="bold">normalization</span>
                                    to it. This process aims to maximize the range of intensities in order to make objects more distinguishable. The formula is \( I_s = \alpha I_e + \beta \), where \(I_s\) is the output image, \(I_e\) the source image.
                                </p>
                                <img style="width: 70%; margin: 0.5rem auto;" src="images/ai/histogram_example.PNG" alt="histogram">
                                <div class="frame-highlight">
                                    <p>What we mean by <span class="bold">dynamic</span> is the range of intensity values that the image contains. The less the dynamic, the harder it is to distinguish objects on the image.
                                        In this case, we say that the image has a <span class="bold">restrained dynamic</span> (left image).
                                        The word dynamic refers also to the <span class="bold">contrast</span> or <span class="bold">depth</span> of the image.
                                        However, the <span class="bold">resolution</span> refers to the number of pixels in the image.
                                    </p>
                                </div>
                                <p>In the example above, we applied a linear transform to go from \( I_e \) to \( I_s \). Nonetheless, we could choose to apply a <span class="bold">non-linear transform</span> to emphasize on a particular range of intensities.
                                    In the screenshot below, the brightest pixels have been more scaled than the darkest ones. This results in a histogram that is more spread on the right side.
                                </p>
                                <img style="width: 70%; margin: 0.5rem auto;" src="images/ai/histogram_example_nonlinear.PNG" alt="histogram">
                                <p>With the histogram, we can also spot where details of an image lie. <span class="bold">Details are located to the lowest occurences</span>. In the image above, details are mainly clustered from the 25th to 100th intensity level.
                                    To make these details more visible, we can apply a <span class="bold">histogram equalization</span>, which evenly spreads the histogram over the whole range of intensities.
                                </p>
                                <div class="frame-highlight">
                                    <p>This technique also amplifies the noise because noise is detail.</p>
                                </div>
                            </div>

                            <div id="object-segmentation"> <h4>Object segmentation</h4>
                                <p>We can use this histogram to distinguish objects from the background. Indeed, by choosing a background that is different from the object's overall intensity,
                                    we shall see 2 bumps on the histogram. This is called a <span class="bold">bi-modal histogram</span>. From this histogram, we can determine a <span class="bold">threshold</span>
                                    that will separate the range of intensities of the object from the background. Then, by applying a <span class="bold">binarization (also called thresholding)</span>, a pixel below the threshold,
                                    is set to 0 and a pixel above the threshold is set to 1. It may also happen that bumps are not clearly set apart, in that case we can use a <span class="bold">hysteresis thresholding</span>.
                                </p>
                                <div class="illustration-horiz-3">
                                    <img style="width: 30%; margin: 0.5rem auto;"" src="images/ai/sic.PNG" alt="sic">
                                    <img style="width: 30%; margin: 0.5rem auto;"" src="images/ai/sic_thresholding.PNG" alt="thresholding">
                                </div>
                                <p>However, this may result in adding an <span class="bold">impulse noise</span> to the image. This can be fixed by filtering the image with a <a href="#non-linear-filter">non-linear filter</a>.
                                    There exist different types of segmentation methods, I will expand on them when I will find the time.
                                </p>
                                <div class="frame-highlight">
                                    <ul>
                                        <li><span class="bold">Histogram methods:</span> Otsu, local thresholding and multpile thresholding, etc.</li>
                                        <li><span class="bold">Region transform methods:</span> Division-Fusion, region growth, etc.</li>
                                        <li><span class="bold">Optimization methods</span></li>
                                    </ul>
                                </div>
                            </div>

                            <div> <h4>Filtering</h4>
                                <div> <h5>Noise</h5>
                                    <p>In image processing, we try as much as possible to get rid of the noise before the actual image processing. This avoids <span class="bold">expensive computations</span>.
                                        There exist 2 types of noise in image processing: <span class="bold">additive</span> and <span class="bold">multiplicative</span>.
                                        In my case, I will only focus on the additive noise as it is the most common one. To modelize the noise, we use probabilities. Therefore, there are different types of noises:
                                    </p>
                                    <div class="frame-highlight">
                                        <ul>
                                            <li> <span class="bold">Impulse noise</span> (or salt-and-pepper).</li>
                                            <li> <span class="bold">Gaussian noise:</span> noise centered on 0 \(N(0,\sigma)\).</li>
                                            <li> <span class="bold">Uniform noise</span></li>
                                            <li> and many more ...</li> 
                                        </ul>                                     
                                    </div>
                                    <p>Depending on which probability law we assume the noise follows, we either use <span class="bold">linear filtering</span> or <span class="bold">non-linear-filtering</span>.
                                    </p>
                                </div>
                                <div> <h5>Linear Filtering</h5>
                                    <p>In 2D, we also have the concept of <span class="bold">convolution</span> that is used to filter an image. The convolution is defined as follows:
                                        \[ I(u,v) = \sum^{u}_{-\infty}\sum^{v}_{-\infty} I(x,y) h(u-x,v-y) dxdy \]
                                        \(I(u,v)\) is the output image, \(I(x,y)\) is the input image, \(h(u,v)\) is the filter and \(u\) and \(v\) are the spatial coordonates.
                                    </p>
                                    <div class="frame-highlight">
                                        <p>In signal processing, the filter's impulse response must be <span class="bold">causal</span>, it is a physical limit. However, <span class="bold">we do not have this constraint</span> for image processing, as
                                            we treat the image in <span class="bold">deferred time (spatial filtering)</span>, thus we can use non-causal filters. Our filter will be centered on the pixel we want to filter using surrounding pixels. 
                                        </p>
                                    </div>
                                    <p>Our filter requires accessing all surrounding pixels, hence pixels at borders are quite unconvenient.
                                        In these cases, we need to extend our image to be able to operate the filter. There exist 3 main techniques:
                                    </p>
                                    <div class="frame-highlight">
                                        <ul>
                                            <li><span class="bold">Zero-Padding:</span> can introduce discontinuities to the image</li>
                                            <li><span class="bold">Image mirroring</span></li>
                                            <li><span class="bold">Image periodization</span></li>
                                        </ul>
                                    </div>
                                    <p>Finally, linear filters are fit for filtering the gaussian noise. The pictures below show the impact of an average filter of size 3x3, 5x5, 7x7 on a noisy image and we can notice that
                                        the noise is somewhat reduced but also widely spread over the image. The best filter for this type of noise is the <span class="bold">median filter</span> which is a non-linear filter.
                                    </p>
                                    <img style="width: 75%;" src="images/ai/filter2_ave_otolithe.PNG" alt="">
                                </div>
                                <div id="non-linear-filter"> <h5>Non-linear Filtering</h5>
                                    <p>To filter <span class="bold">impulse noise</span>, we <span class="bold">cannot use linear filters</span> because instead of diminishing the noise they spread it over the surrounding pixels.
                                        On the other hand, non-linear filters, such as the <span class="bold">median filter</span>, allow to tackle this issue by considering a statistical approach rather than a spectral one.
                                        In the case of the median filter, all pixels contained within the kernel are <span class="bold">ranged by intensity</span>. As a result,
                                        the noise is likely to be the among the lowest or highest values and will be discarded. The median value is chosen as the pixel's output value. 
                                    </p>
                                </div>
                                <img src="images/ai/medfilt2_otolithe.PNG" alt="">
                            </div>

                            <div> <h4>Math morphology</h4>
                                <p>In a context where we need to detect objects on an image, we already saw that we needed to perform a <a href="#object-segmentation">segmentation</a> using <span class="bold">histograms and thresholding</span> (or binarization).
                                    However, it turns out that objects may not be properly segmented. Therefore, we need a method which can correct object's shapes such that they are segmented as we wish.
                                    This is where <span class="bold">math morphology</span> comes in. It has 2 parameters to set: <span class="bold">Application point</span> and <span class="bold">Element's shape</span>.
                                </p>
                                <div class="frame-highlight">
                                    <ul>
                                        <li><span class="bold">Erosion:</span> to erase objects from the image (small imperfections, excrescence ...)</li>
                                        <li><span class="bold">Dilatation:</span> to fill holes in a shape</li>
                                    </ul>
                                    <p>Usually we apply an erosion followed by a dilatation to get rid of small imperfections or noise around objects. This is called an <span class="bold">opening</span>.</p>
                                    <p>We can also apply a dilatation followed by an erosion to fill holes. This is called a <span class="bold">closing</span>.</p>
                                </div>
                                <div> <h5>Connectivity</h5>
                                    <p>In an image, to measure distances we cannot use the <span class="bold">euclidian distance</span> because an image is a grid of pixels. Hence, we use other methods such as the:
                                    </p>
                                    <div class="frame-highlight">
                                        <ul>
                                            <li><span class="bold">4-connectivity:</span> \(|x|+|y|\)</li>
                                            <li><span class="bold">8-connectivity:</span> \(max(x,y)\)</li>
                                        </ul>
                                    </div>
                                    <img src="images/ai/pixel_connectivity.PNG" alt="">
                                </div>
                            </div>

                            <div> <h4>Blur</h4>
                                <p>Usually blur is the <span class="bold">outcome of a low-pass filtering</span>. A low-pass filter widens the <span class="bold">transition area</span> between pixels which leads to reducing the contrast (sharp discontinuities) of objects shapes.
                                    To counter this effect, we can substract the original image to the <span class="bold">Laplacian of the image</span>:
                                    \[ I(x,y) = I(x,y) - \lambda \Delta I(x,y) \]
                                    To have a better understanding of what the Laplacian is, click <a href="math&IT.html">here</a>. In short, this is the second derivative of a 2D function.
                                    Performing this operation on the image's pixels will enhance the contrast of the image.
                                </p>
                                <div class="illustration-horiz-4">
                                    <img src="images/ai/floue.PNG" alt="">
                                    <img src="images/ai/ImR_floue_10Lap.PNG" alt="">
                                    <img src="images/ai/ImR_floue_15Lap.PNG" alt="">
                                    <img src="images/ai/ImR_floue_20Lap.PNG" alt="">
                                </div>
                                <p>The illustrations above show a slightly blurred image and an increase in the \(\lambda\) factor: 10, 15 and 20. We notice an improvement in the contrast of the image.</p>
                            </div>

                            <div> <h4>Edge detection</h4>
                                <p>In an image, edges are the result of high-intensity differences between one or several objects. They can be captured using the derivative of the image along
                                    the \(x\) and \(y\) axis:
                                    \[ \frac{\partial I(x,y)}{\partial x} = \frac{I(x+1,y_0) - I(x-1,y_0)}{2} \equiv \frac{1}{2} \begin{bmatrix} -1 & 0 & 1 \end{bmatrix} \]
                                    \[ \frac{\partial I(x,y)}{\partial y} = \frac{I(x_0,y+1) - I(x_0,y-1)}{2} \equiv \frac{1}{2} \begin{bmatrix} -1 \\ 0 \\ 1 \end{bmatrix} \]
                                    Our purpose though is to emphasize edges, thus we remove the \(\frac{1}{2}\) factor by multiplying those derivatives by 2 to <span class="bold">increase the derivative effect</span>.
                                    However those operations <span class="bold">amplify noise</span> as well, therefore we also need to apply a <span class="bold">smoothing filter</span> before applying the derivative.
                                    This action results in doing the following operations, respectively along the \(x\) and \(y\) axis:
                                    \[ I(x,y) * \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix} * \begin{bmatrix} -1 & 0 & 1 \end{bmatrix} \equiv I(x,y) * \begin{bmatrix} -1 & 0 & 1 \\ -1 & 0 & 1 \\ -1 & 0 & 1 \\ \end{bmatrix} \]
                                    \[ I(x,y) * \begin{bmatrix} 1 & 1 & 1 \end{bmatrix} * \begin{bmatrix} -1 \\ 0 \\ 1 \end{bmatrix} \equiv I(x,y) * \begin{bmatrix} -1 & -1 & -1 \\ 0 & 0 & 0 \\ 1 & 1 & 1 \\ \end{bmatrix} \]
                                </p>
                                <div class="frame-highlight">
                                    <p>The kernels formed above are <span class="bold">Prewitt filters</span> and used to detect edges within an image. There exist vriantes of those filters. One is
                                        the <span class="bold">Sobel filter</span> which uses a <span class="bold">gaussian-like smoothing filter</span> instead of a <span class="bold">rectangle_like smoothing filter</span>:
                                        \[ I(x,y) * \begin{bmatrix} 1 \\ 2 \\ 1 \end{bmatrix} * \begin{bmatrix} -1 & 0 & 1 \end{bmatrix} \equiv I(x,y) * \begin{bmatrix} -1 & 0 & 1 \\ -2 & 0 & 2 \\ -1 & 0 & 1 \\ \end{bmatrix} \]
                                        Indeed, the smoothing filter can introduce <span class="bold">artifacts</span> in the image (consequences in the frequency domain), it should be <span class="bold">as smooth as possible</span>.
                                    </p>
                                </div>
                                <div class="illustration-horiz-3">
                                        <img src="images/ai/trees.PNG" alt="">
                                        <img src="images/ai/sobel_trees.PNG" alt="">
                                </div>
                                <p>After having passed the image through the Sobel filters, we end up with 2 images. Each one accounts for the gradient along the corresponding axis.
                                    We can then combine them to get a <span class="bold">single image</span>. To do so, we can use the <span class="bold">gradient magnitude</span> and the <span class="bold">gradient direction</span>:
                                    \[ m(x,y) = \sqrt{I_x(x,y)^2 + I_y(x,y)^2} \]
                                    \[ \theta(x,y) = arctan(\frac{I_y(x,y)}{I_x(x,y)}) \]
                                    This gradient magnitude \(m(x,y)\) is the <span class="bold">norm of the gradient</span> and is used to <span class="bold">detect edges</span> in an image.
                                    Each \(m(x,y\))'s pixel represents the <span class="bold">intensity of the gradient</span> at this pixel. 
                                    We take each \(m(x,y\))'s pixel and we compare it to its surrounding pixels in the direction of the gradient. If it is the <span class="bold">maximum</span> of its surrounding pixels, then it is an edge and the pixel is conserved.'
                                </p>
                                <p>There are also the <span class="bold">Canny filter</span>, which relies on the first derivative of the gaussian.
                                    \[ I(x,y) = \nabla (I(x,y)*G(x,y)) = (I(x,y)*\nabla G(x,y)) \]
                                    avec \( G(x,y) = e^{-\frac{x²+y²}{2\sigma²}} \). We can notice that this accounts to first <span class="bold">smoothing</span> the image with a gaussian filter and then applying a <span class="bold">derivative filter</span>.
                                    The smoothing part can either be <span class="bold">amplified or dimmed</span> by tweaking the \(\sigma\) factor. The derivative filter is a <span class="bold">Prewitt filter</span> or a <span class="bold">Sobel filter</span>.
                                    Besides, this filter is <span class="bold">separable</span>, hence can be written as \( G(x,y) = G(x)G(y) \), meaning that it can be seen as a 1D filter applied twice, once along the \(x\) axis and once along the \(y\) axis.:
                                    \[ I(x,y) = (I(x,y)*\nabla G(x)) * \nabla G(y) = (I(x,y)* \frac{dG(x)}{dx}) * \frac{dG(y)}{dy} \]
                                    This filter is useful as it can be adjusted to <span class="bold">detect edges of different sizes</span> by changing the \(\sigma\) factor.
                                </p>
                                <div class="frame-highlight">
                                    <p>Edge detection also involves a <span class="bold">hysteresis thresholding</span> with a <span class="bold">low and high</span> threshold. This is also called <span class="bold">contour chaining</span>.</p>
                                </div>
                                <p>Below is an example of the effect of a <span class="bold">canny filter</span> with several values of \(\sigma\) but consistent thresholds We clearly notice that the <span class="bold">higher</span> \(\sigma\), the <span class="bold">less</span> edges are detected.</p>
                                <img style="width: 65%;" src="images/ai/canny_sigmas_trees.PNG" alt="">
                            </div>
                        </div>
                        <hr>
                        <div> <h3>Features extraction</h3>
                            <div> <h4>Hough transform</h4>
                                <p>The <span class="bold">Hough transform</span> was created to recognize shapes in an image such as straight lines, circles and much more intricate ones ...
                                    This transform takes as an input a <span class="bold">black and white image</span> representing contours only and outputs a so-called <span class="bold">accumulator</span>.
                                    The dimension of this accumulator depends on the number of parameters of the shape we are looking for. 
                                </p>
                                <div class="frame-highlight">
                                    <p>For example, if we are looking for straight lines, we only need 2 parameters \( \theta \) and \( \rho \) to describe them. Hence, the accumulator will be a 2D matrix.
                                        If we are looking for circles, we need 3 parameters: the center \( C(x_0,y_0) \) and the radium \( R \) to describe them. Hence, the accumulator will be a 3D matrix.
                                    </p>
                                </div>
                                <p>Below is an example of a accumulator for searching straight lines inside an image. Indeed, we can see both \( \theta \) and \( \rho \) parameters. The brightest points are the most chosen \( \theta \) and \( \rho \) parameters.</p>
                                <img style="width: 30%;" src="images/ai/hough_tab_params.PNG" alt="">
                                <p></p>
                            </div>
                        </div>
                        <hr>
                        <div> <h3>Hands-on image processing</h3>
                            <div> <h4>Shape extraction</h4>
                                <div class="illustration-horiz-4">
                                    <img src="images/ai/mini-proj1.PNG" alt="">
                                    <img style="width: 40%;" src="images/ai/hist_5.1.PNG" alt="">
                                </div>
                                
                                <p>The goal here is to extract the upper-right square and compute its area and its perimeter. The image to the right represents its histogram. To do so, we carry out the following operations on the image:</p>
                                <div class="frame-highlight">
                                    <ol>
                                        <li>Image segmentation: thresholding and binarization (using the histogram).</li>
                                        <li>Opening with a 4x4 square: to erase noisy pixels.</li>
                                        <li>Closing with a 8x8 square: to clog the whole inside the square.</li>
                                        <li>Computing object's perimeter and area using <span class="bold">bwarea</span> and <span class="bold">bwperimeter</span>.</li>
                                    </ol>
                                </div>
                                <p>Below, we can visualize the successive effects of the <span class="bold">opening</span> and <span class="bold">closing</span> operations.</p>
                                <div class="illustration-horiz-2">
                                    <img src="images/ai/opening_square.PNG" alt="">
                                    <img style="width: 30%;" src="images/ai/closing_square.PNG" alt="">
                                </div>
                            </div>
                            <div> <h4>Shape recognition and counting</h4>
                                <img style="width: 30%;" src="images/ai/mini-proj2.PNG" alt="">
                                <p>The goal here is to <span class="bold">count</span> all objects contained in the image and determine whether they have a <span class="bold">circular shape</span>. To do so, we need a perform a sequence of operations, which are the followings:</p>
                                <div class="frame-highlight">
                                    <ol>
                                        <li>Converting the image from RGB to gray color: the shape does not lie in the color.</li>
                                        <li>Segmenting the image: thresholding and binarization.</li>
                                        <li>Labelling objects: filling and labbeling(isolating each shape with a number).</li>
                                        <li>Shape criterion: find a criterion to conclude on the nature of shape.</li>
                                    </ol>
                                </div>
                                <p></p>
                                <img src="images/ai/sobel_prewitt,canny_log.PNG" alt="">
                            </div>
                            <div> <h4>Insight of a doctorate project</h4>
                                <p>In </p>
                            </div>
                            <div> <h4>Tools for image processing</h4>
                                <div> <h5>Matlab</h5>
                                    <p>On Matlab, we can use the function <span class="bold">graycomatrix</span> to compute the co-occurrence matrix and the function <span class="bold">graycoprops</span> to compute the its <span class="bold">Haralick vector</span>.
                                    </p>
                                    <ul>
                                        <li>imshow: display an image</li>
                                        <li>imagesc: display the result of a mathematical operation on the image (pixels are not "real" pixels and can take a negative value)</li>
                                    </ul>
                                    <p></p>
                                </div>
                                <div> <h5>OpenCV</h5>
                                    <p>I work with Google Colab. I will upload soon a notebook with some examples.</p>
                                </div>
                            </div>
                        </div>
                        <div> <h3>TensorFlow and embedded computer vision</h3>
                            <p>Besides this university course, I have taken on a personal project which is to learn and implement AI solutions for computer vision.
                                I am currently learning AI using the <a href="ai.html">TensorFlow python library</a> and plan to do carry out a <a href="currentprojects.html">project</a> during the winter holidays combining <span class="bold">AI and embedded systems</span>.
                            </p>
                        </div>
                    </div>
                </div>

                <div class="sub-container"> <div class="label"> <h2>Digital Embedded Systems</h2> </div>
                    <div class="keywords">OS architecture, performances, parallelism paradigm, semaphores, FreeRTOS</div>
                    <p>You can consult some <a href="currentprojects.html" target="_parent">projects</a> done as part of this course.</p>
                    <div class="content">
                        <p style="font-style: italic;">In this course, we had several <a href="currentprojects.html">projects</a>.</p>
                        <!-- <div class="subject-subdiv"> <h3>1. Architectures</h3>
                            <p>Parallelism</p>
                        </div> -->
                    </div>
                </div>

                <div class="sub-container"> <div class="label"> <h2>Network & Communication Systems</h2> </div>
                    <div class="keywords">Networking concepts, UDP / TCP, HTTPS , Sockets, CAN / UART / I²C / SPI</div>
                    <div class="content">
                        <p>This course is organised around lab sessions. All the experimentations have been done with C++. You can find more by clicking <a href="network.html">here</a>.</p>
                        <hr>
                        <div class="subject-subdiv"> <h3>1. Fundamentals: Addressing / Routing / Firewall</h3>
                            <p>In these lab sessions, we basically covered :</p>
                            <div class="frame-highlight">
                                <ol>
                                    <li> <span class="bold">The difference between hubs and switchs;</span> </li>
                                    <li> <span class="bold">How to configure a network interface;</span> </li>
                                    <li> <span class="bold">Basic protocols such as ICMP and ARP (MAC & IP addresses);</span> </li>
                                    <li> <span class="bold">OSI and TCP/IP models, encapsulation of data, subnet-mask;</span> </li>
                                    <li> <span class="bold">Reachability, hierarchial network structure;</span> </li>
                                    <li> <span class="bold">IP forwarding , routing table;</span> </li>
                                    <li> <span class="bold">TTL (Time To Live)</span> </li>
                                    <li> <span class="bold">SNAT, DNAT;</span> </li>
                                    <li> <span class="bold">Firewall (filtering network traffic).</span> </li>
                                </ol>
                            </div>
                        </div>
                        <hr>
                        <div class="subject-subdiv"> <h3>2. Process & Sockets</h3>
                            <p>In these lab sessions, we basically covered :</p>
                            <div class="frame-highlight">
                                <ol>
                                    <li> <span class="bold">Multi-threading, pipe and syscalls;</span> </li>
                                    <li> <span class="bold">UDP sockets;</span> </li>
                                    <li> <span class="bold">TCP sockets;</span> </li>
                                    <li> <span class="bold">Sockets performances.</span> </li>
                                </ol>
                            </div>
                        </div>
                        <hr>
                        <div class="subject-subdiv"> <h3>3. Webserver</h3>
                            <p>In these lab sessions, we basically covered :</p>
                            <div class="frame-highlight">
                                <ol>
                                    <li> <span class="bold">HTTP, HTTPS and websockets;</span> </li>
                                    <li> <span class="bold">SSL procedure;</span> </li>
                                    <li> <span class="bold">HTTP performances;</span> </li>
                                    <li> <span class="bold">Basic web app using JS.</span> </li>
                                </ol>
                            </div>
                        </div>
                        <hr>
                        <div class="subject-subdiv"> <h3>4. Physical layer</h3>
                            <p>In these lab sessions, we basically covered :</p>
                            <div class="frame-highlight">
                                <ol>
                                    <li>...</li>
                                </ol>
                            </div>
                        </div>
                    </div>
                </div>

                <div class="sub-container"> <div class="label"> <h2>Interface Power Systems</h2> </div>
                    <div class="keywords">Closed-loop / feedback systems, sensors, motors, power electronics</div>
                    <div class="content">
                        <p>This course aimed to treat the use of electronics components to build various sensors. This course was structured around this <a href="ips_proj.html">project</a></p>
                        <div class="subject-subdiv"> <h3></h3>
                            <p>In this course, we basically covered :</p>
                            <div class="frame-highlight">
                                <ol>
                                    <li> <span class="bold">Sensors;</span> </li>
                                    <li> <span class="bold">Filtering;</span> </li>
                                    <li>...</li>
                                </ol>
                            </div>
                        </div>
                    </div>
                </div>

                <div class="sub-container"> <div class="label"> <h2>Company administration & management</h2> </div>
                    <p style="font-style: italic;">The following section will be in french because I do not know precisely the vocabulary related to finance and management.</p>
                    <div class="keywords">Comptabilité française, Finance, Gestion, Management</div>
                    <div class="content">
                        <p>Ce cours couvre les fondamentaux de la gestion comptable d'une entreprise.</p>
                        <hr>
                        <div class="subject-subdiv"> <h3>1. Bilan, Compte de Résultat et Flux de Trésorerie</h3>
                            Le schéma ci-dessous (en cours d'élaboration) résume les principaux éléments de ses documents.
                            <img src="images/MAE_Chap1.svg" alt="mae">
                        </div>
                        <hr>
                        <div class="subject-subdiv"> <h3>2. TVA</h3>
                            <p>La TVA est neutre pour les entreprises. Elles jouent le rôle de percepteur auprès de l'état. Lorsqu'une entreprise vend ses services ou produits, elle doit reverser 20% de
                                ses produits à l'état (TVA due = dettes auprès de l'état), lorsqu'elle achète des services ou marchandises auprès d'autres entreprises, elle les paie T.T.C. mais 20% de ses charges sont
                                remboursées par l'état (TVA déductible = créances auprès de l'état).
                            </p>
                        </div>
                        <hr>
                        <div class="subject-subdiv"> <h3>3. Sources d'investissements</h3>
                            <p>R&D</p>
                            <div> <h5>Source d'investissements internes</h5>
                                <p>Capital social, apport au compte courant des associés, autofinancement, subventions, etc.</p>
                                <p>Lors d'une augmentation de capital, l'entreprise réémet des actions dont la <span class="bold">prime d'émission</span> est fixée par la formule : \(V = (PE-VN) \tims nb_{TE}\)</p>
                                <div class="frame-highlight">
                                    <p>Une entrerpise possède un CP de départ de 100 000€ constitué de 10000 actions dont la valeur nominale est 10€. Après quelques années, elle a accumulé de la richesse et a augmenté ses capitaux propres de 50 000€.
                                        Elle est donc estimée à 150 000€. Elle décide alors de réémettre des actions pour augmenter son capital social (toujours de 100 000€). Le <span class="bold">prix d'émission</span> d'une nouvelle action est alors de \( \frac{CP}{nb_{actions}} = \frac{150000}{10000} = 15\)€.
                                        La <span class="bold">prime d'émission</span> est donc de 15-10 = 5€. La prime d'émission est enregistrée à part dans le bilan de l'entreprise (voir illustration ci-dessous).
                                    </p>
                                </div>
                                <img src="images/MAE_Chap4.svg" alt="MAE_Chap4">
                            </div>
                            <div> <h5>Source d'investissements externes</h5>
                                <p>Emprunt banquaire, LOA (Location avec Option d'Achat)</p>
                            </div>
                            <div> <h5>Actions VS Obligations</h5>
                                <p>Afin d'obtenir des sources d'investissements externes, une entreprise peut décider de mettre à disposition des actions ou des obligations.
                                    Une <span class="bold">action</span> sont des titres qui consituent le capital social de l'entreprise, elle permet d'acquérir un <span class="bold">pouvoir de décision</span> dans l'entreprise (droit de vote) ainsi qu'un <span class="bold">droit à dividendes</span>.
                                    L'entreprise <span class="bold">n'a pas comme obligation légale</span> de rembourser ses actionnaires. D'un autre côté, une <span class="bold">obligation</span> diffère d'une action en se sens que l'entreprise s'engage à la rembourser à une <span class="bold">date donnée</span> avec un <span class="bold">taux d'intérêt</span> fixé à l'avance.
                                    Cependant, les obligations ne donnent <span class="bold">pas de pouvoir de décision</span> dans l'entreprise. En émettant des obligations, <span class="bold">c'est comme si l'entreprise s'endettait auprès de particuliers ou d'entreprises</span>.
                                    L'avantage est qu'elle peut déterminer <span class="bold">elle-même</span> les conditions de remboursements (date limite et intérêts). En résumé :
                                </p>
                                <div class="frame-highlight">
                                    <ul>
                                        <li><span class="bold">Action :</span> dividendes (en fonction des bénéfices), droit de vote et potentiel gain de valeur à la revente (si l'entreprise à accumuler beaucoup de richesses).</li>
                                        <li><span class="bold">Obligation :</span> remboursement après une date limite (avec intérêts à taux fixe) mais pas de droit de vote.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                        <hr>
                        <div class="subject-subdiv"> <h3>4. Dotations</h3>
                            <p>La <span class="bold">dotation</span> constitue une action comptable qui a pour but de prendre en compte la <span class="bold">perte de valeur</span> certaine ou probable, d'un bien matériel ou immatériel.
                                Elle s'inscrit dans les principes comptables de <span class="bold">prudence</span> et <span class="bold">d'image fidèle</span>. Il existe plusieurs sortes de dotations :
                            </p>
                            <div class="frame-highlight">
                                <ul>
                                    <li>Les <span class="bold">ammortissements</span> qui représentent les pertes de valeurs certaines d'actifs telles que l'usure du matériel.</li>
                                    <li>Les <span class="bold">dépréciations</span> qui représentent les pertes de valeurs probables d'actifs tels que les actions, les obligations, l'incertitude de rembousement des créances, etc.</li>
                                    <li>Les <span class="bold">provisions</span> qui représentent les dettes probables telles ques les amendes, les procès, etc.</li>
                                </ul>
                            </div>
                            <p>Elle se calcule toujours par rapport au <span class="bold">coût d'achat H.T.</span> (soit la valeur réelle du bien). D'un bilan à l'autre, il y a <span class="bold">réévaluations</span> (aussi réajustement) des biens que l'entreprise possède.
                                Ces biens peuvent être réévalués à la hausse (plus value), ce qui conduit à une <span class="bold">reprise</span>, ou à la baisse (moins value), ce qui conduit à une <span class="bold">nouvelle dotation</span>.
                                Les reprises et dotations apparaissent dans le <span class="bold">CDR</span> car ils sont compris comme étant des <span class="bold">produits ou des charges exceptionnelles</span>.
                                En revanche, le bilan fait paraître la <span class="bold">valeur originale</span>, la <span class="bold">valeur de la dotation actuelle</span> et la <span class="bold">valeur nette comptable</span>.
                            </p>
                            <img src="images/MAE_Chap5_6.svg" alt="MAE_Chap5_6">
                        </div>
                        <hr>
                        <div class="subject-subdiv"> <h3>5. Évaluation des entreprises</h3>
                            <p>Pour déterminer la valeur d'une entreprise, il existe différentes méthodes. Nous nous contenterons des trois principales méthodes d'évauation, que sont :</p>
                            <div class="frame-highlight">
                                <ul>
                                    <li><span class="bold">Valeur patrimoniale :</span> valeur de l'ensemble des richesses accumuluées (passé) \( \rightarrow V = CP + PV - MV - NV - D \),
                                        avec CP (capital social), PV et MV (+/- value), NV (non-valeurs) et D (dettes).</li>
                                    <li><span class="bold">Valeur de rendement :</span> valeur prospective, probable (futur) \( \rightarrow V = \sum_{k=1}^N \frac{R_k}{(1+i)^k}\),
                                        avec N le nombre d'années de projection, \(R_k\) le rendement de l'année \(k\) et \(i\) le taux <span class="bold">d'actualisation</span> qui est un indicateur de l'estimation du gain futur.</li>
                                    </li>
                                    <li><span class="bold">Valeur comparative :</span> valeur par comparaison des valeurs des autres entreprises</li>
                                </ul>
                            </div>
                        </div>
                        <hr>
                        <div class="subject-subdiv"> <h3>7. Calcul des coûts complets d'un produit par la méthode des centres d'analyses</h3>
                            <p>Afin de déterminer, les coûts complets d'un produit, nous utilisons la méthode des centres d'analyses. Elle consiste en 8 étapes:</p>
                            <div class="frame-highlight">
                                <span class="bold">Répartition des charges indirectes</span>
                                <ol>
                                    <li>Répartition primaire: charges indirectes réparties sur les centres principaux et de structures.</li>
                                    <li>Répartition secondaire: ajout des charges des centres auxiliaires réparties sur les centres principaux .</li>
                                    <li>Coût d'une Unité d'oeuvre (UO): répartition secondaire finale * quantité d'UO.</li>
                                </ol>
                            </div>
                            <!-- <p></p> -->
                            <div class="frame-highlight">
                                <span class="bold">Coût d'achat</span>
                                <p>Cela couvre l'ensemble des processus impliqués dans l'acheminement de biens nécessaires à la fabrication d'un produit: 
                                Charges directes + Charges indirectes liées à l'achat (approvisionnement, réception livraisons ...).
                                </p>
                                <ul>
                                    <li>Prix d'achat nets des MP.</li>
                                    <li>Charges de main d'oeuvre directe (pas la main d'oeuvre employée pour la fabrication du produit !).</li>
                                    <li>Accessoires d'achats: transport.</li>
                                </ul>
                            </div>
                            <p></p>
                            <div class="frame-highlight">
                                <span class="bold">Etude des stocks de matières premières (MP)</span>
                                <ol>
                                    <li>Stock initial / Stock consommé: Quantité, coût unitaire, valeur.</li>
                                    <li>Achat début mois / Stock final: Quantité, coût unitaire, valeur.</li>
                                </ol>
                            </div>
                            <p></p>
                            <div class="frame-highlight">
                                <span class="bold">Coût de production</span>
                                <ol>
                                    <li>Stock initial / Stock consommé: Quantité, coût unitaire, valeur.</li>
                                    <li>Production du mois / Stock final: Quantité, coût unitaire, valeur.</li>
                                </ol>
                            </div>
                            <p></p>
                            <div class="frame-highlight">
                                <span class="bold">Etude des stocks de prdouits finis (PF)</span>
                                <ol>
                                    <li>Stock initial / Stock vendus: Quantité, coût unitaire, valeur.</li>
                                    <li>Production du mois / Stock final: Quantité, coût unitaire, valeur.</li>
                                </ol>
                            </div>
                            <p></p>
                            <div class="frame-highlight">
                                <span class="bold">Coût de production</span>
                                <ol>
                                    <li>Stock initial / Stock consommé: Quantité, coût unitaire, valeur.</li>
                                    <li>Production du mois / Stock final: Quantité, coût unitaire, valeur.</li>
                                </ol>
                            </div>
                            <p></p>
                            <div class="frame-highlight">
                                <span class="bold">Calcul du coût de revient</span>
                                <p>Il s'agit du montant nécessaire à dépenser pour l'entreprise afin de fabriquer un produit:
                                    Coût des PF vendus + Coût hors-production
                                </p>
                            </div>
                        </div>
                    </div>
                </div>

            </section> <!-- accordion -->
        
        </section> <!-- main-container -->

        <div id="footer"></div>

    </body>

</html>